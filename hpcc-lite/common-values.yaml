global:
  # Settings in the global section apply to all HPCC components in all subcharts

  image:
    ## It is recommended to name a specific version rather than latest, for any non-trivial deployment
    ## For best results, the helm chart version and platform version should match, which is the default if version is
    ## not specified. Do not override without good reason as undefined behavior may result. 
    ## version: x.y.z
    root: "hpccsystems"    # change this if you want to pull your images from somewhere other than DockerHub hpccsystems
    pullPolicy: IfNotPresent
    ## If you need to provide credentials to pull your image, they should be added as a k8s secret, and the secret name provided here
    # imagePullSecrets: xxx

  ## busybox image is used for some initialization/termination tasks - you can override the location here
  #busybox: "myrepo/busybox:stable"

  ## It is possible (but not recommended) to change the uid/gid that the HPCC containers run under
  ## user:
  ##  uid: 10000
  ##  gid: 10001

  # logging sets the default logging information for all components. Can be overridden locally
  logging:
    detail: 80

  ## resource settings for stub components
  #stubInstanceResources:
  #  memory: "200Mi"
  #  cpu: "20m"

  ## env adds default environment variables for all components. Environment settings can also be added or overridden locally
  #env:
  #- name: SMTPserver
  #  value: mysmtpserver

  # Specify a defaultEsp to control which eclservices service is returned from Std.File.GetEspURL, and other uses
  # If not specified, the first esp component that exposes eclservices application is assumed.
  # Can also be overridden locally in individual components
  ## defaultEsp: eclservices
  
  egress:
    ## If restricted is set, NetworkPolicies will include egress restrictions to allow connections from pods only to the minimum required by the system
    ## Set to false to disable all egress policy restrictions (not recommended)
    restricted: true
    
    ## The kube-system namespace is not generally labelled by default - to enable more restrictive egress control for dns lookups we need to be told the label
    ## If not provided, DNS lookups on port 53 will be allowed to connect anywhere
    ## The namespace may be labelled using a command such as "kubectl label namespace kube-system name=kube-system"
    # kubeSystemLabel: "kube-system"

    ## To properly allow access to the kubectl API from pods that need it, the cidr of the kubectl endpoint needs to be supplied
    ## This may be obtained via "kubectl get endpoints --namespace default kubernetes"
    ## If these are not supplied, egress controls will allow access to any IPs/ports from any pod where API access is needed
    # kubeApiCidr: 172.17.0.3/32
    # kubeApiPort: 7443

    ## named egress sections defined here, can be referenced by components, or they can define their own egress section explicitly
    #engineEgress:
    #- to:
    #  - ipBlock:
    #      cidr: 10.9.8.7/32
    #  ports:
    #  - protocol: TCP
    #    port: 443


  cost:
    currencyCode: USD
    # The following are example pricing based on standard Azure pricing and should be updated to reflect actual rates
    perCpu: 0.0565000000001        # D64ds_v4 compute node ($2,639.68/month for 64 vCPU)
    storageAtRest: 0.0208000000001 # Blob storage pricing (East US/Flag NS/LRS redundancy/Hot)
    storageReads: 0.00400000000001 # Blob storage pricing (East US/Flag NS/LRS redundancy/Hot)
    storageWrites: 0.0500000000001 # Blob storage pricing (East US/Flag NS/LRS redundancy/Hot)

  # postJobCommand will execute at the end of a dynamically launched K8s job,
  # when the main entrypoint process finishes, or if the readiness probes trigger a preStop event.
  # This can be useful if injected sidecars are installed that need to be told to stop.
  # If they are not stopped, the pod continues running with the side car container only, in a "NotReady" state.
  # An example of this is the Istio envoy side car. It can be stopped with the command below.
  # Set postJobCommandViaSidecar to true, if the command needs to run with privilege, this will enable the command
  # to run as root in a sidecar in same process space as other containers, allowing it to for example send signals
  # to processes in sidecars
  # misc:
  #   postJobCommand: "curl -sf -XPOST http://127.0.0.1:15020/quitquitquit"
  # Or example for linkerd
  #   postJobCommand: "kill $(pgrep linkerd2-proxy)"
  #   postJobCommandViaSidecar: true

  ## visibilities section can be used to set labels, annotations and service type for any service with the specified visibility
  visibilities:
    cluster:
      type: ClusterIP
    local:
      annotations:
        # This annotation will make azure load balancer use an internal rather than an internet-visible address
        # May want different values on different cloud providers or use-cases. For example on AWS you may want to use
        #service.beta.kubernetes.io/aws-load-balancer-internal: "true"
        service.beta.kubernetes.io/azure-load-balancer-internal: "true"
      type: LoadBalancer
      # If ingress is specified, an ingress Network Policy will be created for any pod implementing a service with this visibility
      # Default allows ingress from anywhere, but more restrictive rules can be used if preferred.
      # Ingress rules can also be overridden by individual services
      ingress:
      - {}
    global:
      #labels:
      #  mylabel: "4"
      type: LoadBalancer
      ingress:
      - {}
      ## CIDRS allowed to access this service.
      #loadBalancerSourceRanges: [1.2.3.4/32, 5.6.7.8/32]
      
      
# For pod placement instruction and examples please reference docs/placements.md
# The following is for tolerations of Spot Node Pool on Azure. Other cloud providers
# may have different taints for Spot Node Pool. The tolerations are harmless when
# there is no taint on the node pool.
#placements:
# - pods: ["all"]
#   placement:
#     tolerations:
#     - key: "kubernetes.azure.com/scalesetpriority"
#       operator: "Equal"
#       value:  "spot"
#       effect: "NoSchedule"

## The secrets section contains a set of categories, each of which contain a list of secrets.  The categories determine which
## components have access to the secrets.
## For each secret:
##   name is the name that it is accessed by within the platform
##   secret is the name of the secret that should be published
secrets:
  #timeout: 300 # timeout period for cached secrets.  Should be similar to the k8s refresh period.

  #Secret categories follow, remove the {} if a secret is defined in a section
  storage: {}
    ## Secrets that are required for accessing storage.  Currently exposed in the engines, but in the future will
    ## likely be restricted to esp (when it becomes the meta-data provider)
    ## For example, to set the secret associated with the azure storage account "mystorageaccount" use
    ##azure-mystorageaccount: storage-myazuresecret

  authn: {}
    ## Category to deploy authentication secrets to container, and to create a key name alias to reference those secrets
    #ldapadmincredskey: "admincredssecretname"  ## Default k/v for LDAP authentication secrets
    #testauthusercreds1: "testauthusercreds1"            ## Default k/v for test authentication secrets
    #testauthusercreds2: "testauthusercreds2"            ## Default k/v for test authentication secrets

  ecl: {}
    ## Category for secrets published to all components that run ecl.  These secrets are for use by internal
    ## ECL processing.  For example HTTPCALL and SOAPCALL have built in support for secrets that are not directly
    ## accessible to users, that is, accessed directly via ECL code.

  eclUser: {}
    ## Category for secrets accessible via ecl code.  These are secrets that users can access directly.  Be cautious about
    ## what secrets you add to this category as they are easily accessed by ECL code.

  codeSign: {}
    #gpg-private-key-1: codesign-gpg-key-1
    #gpg-private-key-2: codesign-gpg-key-2

  codeVerify: {}
    #gpg-public-key-1: codesign-gpg-public-key-1
    #gpg-public-key-2: codesign-gpg-public-key-2

  system: {}
    ## Category for secrets published to all components for system level useage

  git: {}
    ## Category to provide passwords for eclccserver to access private git repos


dali:
- name: mydali
  auth: none
  services: # internal house keeping services
    coalescer:
      service:
        servicePort: 8877
      #interval: 2 # (hours)
      #at: "* * * * *" # cron type schedule, i.e. Min(0-59) Hour(0-23) DayOfMonth(1-31) Month(1-12) DayOfWeek(0-6)
      #minDeltaSize: 50 # (Kb) will not start coalescing until delta log is above this threshold
      #resources:
      #  cpu: "1"
      #  memory: "1G"

  #resources:
  #  cpu: "1"
  #  memory: "4G"

esp:
- name: eclwatch
  ## Pre-configured esp applications include eclwatch, eclservices, and eclqueries
  application: eclwatch
  auth: none
  replicas: 1
  ## The following 'corsAllowed' section is used to configure CORS support
  ##   origin - the origin to support CORS requests from
  ##   headers - the headers to allow for the given origin via CORS
  ##   methods - the HTTP methods to allow for the given origin via CORS
  ##
  #corsAllowed:
  ## origin starting with https will only allow https CORS
  #- origin: https://*.my.com
  #  headers:
  #  - "X-X"
  #  methods:
  #  - "GET"
  #  - "OPTIONS"
  ## origin starting with http will allow http or https CORS
  #- origin: http://www.example.com
  #  headers:
  #  - "*"
  #  methods:
  #  - "GET"
  #  - "POST"
  #  - "OPTIONS"

# Add remote clients to generated client certificates and make the ESP require that one of the generated certificates is provided by a client in order to connect
#   When setting up remote clients make sure that certificates.issuers.remote.enabled is set to true.
# remoteClients:
# - name: petfoodApplicationProd
#   organization: petfoodDept
#   secretTemplate:
#     annotations:
#       kubed.appscode.com/sync: "hpccenv=petfoodAppProd" # use kubed config-syncer to replicate certificate to namespace with matching annotation (also supports syncing with separate aks clusters)

  service:
    ## port can be used to change the local port used by the pod. If omitted, the default port (8880) is used
    port: 8888
    ## servicePort controls the port that this service will be exposed on, either internally to the cluster, or externally
    servicePort: 8010
    ## wsdlAddress should be set to the host and port which clients can use to hit this service.
    #   This address is added to the service wsdl files which simplify setting up a SOAP client to hit this service.  There may be many external factors determining the address
    #   that is accible to clients.
    # wsdlAddress: clientfacingaddress:8010
    ## Specify visibility: local (or global) if you want the service available from outside the cluster. Typically, eclwatch and wsecl are published externally, while eclservices is designed for internal use.
    visibility: global
    ## Annotations can be specified on a service - for example to specify provider-specific information such as service.beta.kubernetes.io/azure-load-balancer-internal-subnet
    #annotations:
    #  service.beta.kubernetes.io/azure-load-balancer-internal-subnet: "mysubnet"
    #  The service.annotations prefixed with hpcc.eclwatch.io should not be declared here. They can be declared
    #  in other services in order to be exposed in the ECLWatch interface. Similar function can be used by other 
    #  applications. For other applications, the "eclwatch" inside the service.annotations should be replaced by
    #  their application names. 
    #  hpcc.eclwatch.io/enabled: "true"
    #  hpcc.eclwatch.io/description: "some description"
    ## You can also specify labels on a service
    #labels:
    #  mylabel: "3"
    ## Links specify the web links for a service. The web links may be shown on ECLWatch.
    #links:
    #- name: linkname
    #  description: "some description"
    #  url: "http://abc.com/def?g=1"
    ## CIDRS allowed to access this service.
    #loadBalancerSourceRanges: [1.2.3.4/32, 5.6.7.8/32]
  # Increase maxRequestEntityLength when query deployments (or similar actions) start to fail because they surpass the maximum size
  #  default for EclWatch is 60M, default for other services is 8M
  #maxRequestEntityLength: 70M
  #resources:
  #  cpu: "1"
  #  memory: "2G"
- name: eclservices
  application: eclservices
  auth: none
  replicas: 1
  service:
    servicePort: 8010
    visibility: cluster
  # Increase maxRequestEntityLength when query deployments (or similar actions) start to fail because they surpass the maximum size
  #  default for EclWatch is 60M, default for other services is 8M
  #maxRequestEntityLength: 9M
  #resources:
  #  cpu: "250m"
  #  memory: "1G"
  
sasha: 
 disabled: true